{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datetime in /opt/conda/lib/python3.6/site-packages (4.3)\n",
      "Requirement already satisfied: zope.interface in /opt/conda/lib/python3.6/site-packages (from datetime) (5.4.0)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.6/site-packages (from datetime) (2017.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from zope.interface->datetime) (38.4.0)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from zipfile import ZipFile\n",
    "import configparser\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "!pip3 install datetime\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AKIAV565JVOFR22OXGG5\n",
      "70e/pw0lw1YwOy5njkRVsnqIC5F5EH25lWzRs2hk\n",
      "s3a://udacity-dend/song_data/*/*/*/*.json\n"
     ]
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "song_data = config.get('S3_Input', 'Song_Data')\n",
    "song_data_test = config.get('S3_Input', 'Song_Data_Test')\n",
    "\n",
    "log_data = config.get('S3_Input', 'Log_Data')\n",
    "\n",
    "print(os.environ['AWS_ACCESS_KEY_ID'])\n",
    "print(os.environ['AWS_SECRET_ACCESS_KEY'])\n",
    "print(song_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAAAK128F9318786.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAAAV128F421A322.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAABD128F429CF47.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAACN128F9355673.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAAEA128F935A30D.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAAED128E0783FAB.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAAEM128F93347B9.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAAEW128F42930C0.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAAFD128F92F423A.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAAGR128F425B14B.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAAHD128F42635A5.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAAHJ128F931194C.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAAHZ128E0799171.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAAIR128F1480971.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAAJN128F428E437.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAAND12903CD1F1B.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAANK128F428B515.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAAOF128F429C156.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAAPK128E0786D96.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAAQN128F9353BA0.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAAQO12903CD8E1C.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAAUC128F428716F.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAAUR128F428B1FA.json')\n",
      "s3.ObjectSummary(bucket_name='udacity-dend', key='song_data/A/A/A/TRAAAYL128F4271A5B.json')\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket('udacity-dend')\n",
    "\n",
    "for file in my_bucket.objects.filter(Prefix='song_data/A/A/A/'):\n",
    "    print(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2021-06-04-19-59-19-954342'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.programiz.com/python-programming/datetime/strftime\n",
    "\n",
    "date_created = dt.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "date_created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_id</th>\n",
       "      <th>artist_latitude</th>\n",
       "      <th>artist_location</th>\n",
       "      <th>artist_longitude</th>\n",
       "      <th>artist_name</th>\n",
       "      <th>duration</th>\n",
       "      <th>num_songs</th>\n",
       "      <th>song_id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARTC1LV1187B9A4858</td>\n",
       "      <td>51.45360</td>\n",
       "      <td>Goldsmith's College, Lewisham, Lo</td>\n",
       "      <td>-0.01802</td>\n",
       "      <td>The Bonzo Dog Band</td>\n",
       "      <td>301.40036</td>\n",
       "      <td>1</td>\n",
       "      <td>SOAFBCP12A8C13CC7D</td>\n",
       "      <td>King Of Scurf (2007 Digital Remaster)</td>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ARA23XO1187B9AF18F</td>\n",
       "      <td>40.57885</td>\n",
       "      <td>Carteret, New Jersey</td>\n",
       "      <td>-74.21956</td>\n",
       "      <td>The Smithereens</td>\n",
       "      <td>192.52200</td>\n",
       "      <td>1</td>\n",
       "      <td>SOKTJDS12AF72A25E5</td>\n",
       "      <td>Drown In My Own Tears (24-Bit Digitally Remast...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ARSVTNL1187B992A91</td>\n",
       "      <td>51.50632</td>\n",
       "      <td>London, England</td>\n",
       "      <td>-0.12714</td>\n",
       "      <td>Jonathan King</td>\n",
       "      <td>129.85424</td>\n",
       "      <td>1</td>\n",
       "      <td>SOEKAZG12AB018837E</td>\n",
       "      <td>I'll Slap Your Face (Entertainment USA Theme)</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AR73AIO1187B9AD57B</td>\n",
       "      <td>37.77916</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>-122.42005</td>\n",
       "      <td>Western Addiction</td>\n",
       "      <td>118.07302</td>\n",
       "      <td>1</td>\n",
       "      <td>SOQPWCR12A6D4FB2A3</td>\n",
       "      <td>A Poor Recipe For Civic Cohesion</td>\n",
       "      <td>2005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ARXQBR11187B98A2CC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Liverpool, England</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Frankie Goes To Hollywood</td>\n",
       "      <td>821.05424</td>\n",
       "      <td>1</td>\n",
       "      <td>SOBRKGM12A8C139EF6</td>\n",
       "      <td>Welcome to the Pleasuredome</td>\n",
       "      <td>1985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            artist_id  artist_latitude                    artist_location  \\\n",
       "0  ARTC1LV1187B9A4858         51.45360  Goldsmith's College, Lewisham, Lo   \n",
       "1  ARA23XO1187B9AF18F         40.57885               Carteret, New Jersey   \n",
       "2  ARSVTNL1187B992A91         51.50632                    London, England   \n",
       "3  AR73AIO1187B9AD57B         37.77916                  San Francisco, CA   \n",
       "4  ARXQBR11187B98A2CC              NaN                 Liverpool, England   \n",
       "\n",
       "   artist_longitude                artist_name   duration  num_songs  \\\n",
       "0          -0.01802         The Bonzo Dog Band  301.40036          1   \n",
       "1         -74.21956            The Smithereens  192.52200          1   \n",
       "2          -0.12714              Jonathan King  129.85424          1   \n",
       "3        -122.42005          Western Addiction  118.07302          1   \n",
       "4               NaN  Frankie Goes To Hollywood  821.05424          1   \n",
       "\n",
       "              song_id                                              title  year  \n",
       "0  SOAFBCP12A8C13CC7D              King Of Scurf (2007 Digital Remaster)  1972  \n",
       "1  SOKTJDS12AF72A25E5  Drown In My Own Tears (24-Bit Digitally Remast...     0  \n",
       "2  SOEKAZG12AB018837E      I'll Slap Your Face (Entertainment USA Theme)  2001  \n",
       "3  SOQPWCR12A6D4FB2A3                   A Poor Recipe For Civic Cohesion  2005  \n",
       "4  SOBRKGM12A8C139EF6                        Welcome to the Pleasuredome  1985  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# song_data_test used for quick testing\n",
    "# Use song_data for spark EMR\n",
    "test_file = spark.read.json(song_data_test)\n",
    "\n",
    "test_file.printSchema()\n",
    "test_file.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Load Songs Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "with ZipFile('data/song-data.zip', 'r') as zipObj:\n",
    "   # Extract all the contents of zip file in current directory\n",
    "   zipObj.extractall('data/song-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "song_data_df = spark.read.json(song_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "song_data_df.printSchema()\n",
    "song_data_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "song_pandas = song_data_df.toPandas()\n",
    "song_pandas['song_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Create Song and Artist Tables\n",
    "## Output tables in Parquet Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Song Data Table parquet file\n",
    "song_data_df.createOrReplaceTempView(\"song_data_DF\")\n",
    "\n",
    "song_table = spark.sql(\"\"\"\n",
    "    SELECT  DISTINCT song_id, \n",
    "            title, \n",
    "            artist_id, \n",
    "            year, \n",
    "            duration\n",
    "    FROM song_data_DF\n",
    "    ORDER BY song_id\n",
    "    \"\"\")\n",
    "\n",
    "song_table.printSchema()\n",
    "song_table.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "song_table.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('data/output_data'):\n",
    "    os.makedirs('data/output_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#https://sparkbyexamples.com/pyspark/pyspark-read-and-write-parquet-file/\n",
    "    \n",
    "\n",
    "song_table.write.mode(\"overwrite\").partitionBy(\"year\", \"artist_id\").parquet(\"/data/output_data/song_table.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "song_table_read = spark.read.parquet(\"/data/output_data/song_table.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "song_table_read.printSchema()\n",
    "song_table_read.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "song_table_read.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "song_data_df.createOrReplaceTempView(\"artist_data_DF\")\n",
    "\n",
    "artist_table = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        DISTINCT artist_id,\n",
    "                 artist_name as name,\n",
    "                 artist_location as location,\n",
    "                 artist_latitude as latitude,\n",
    "                 artist_longitude as longitude\n",
    "        FROM artist_data_DF\n",
    "        ORDER BY artist_id\n",
    "                 \"\"\")\n",
    "\n",
    "artist_table.printSchema()\n",
    "artist_table.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "artist_table.write.mode(\"overwrite\").parquet(\"/data/output_data/artist_table.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "artist_table_df = spark.read.parquet(\"/data/output_data/artist_table.parquet\")\n",
    "\n",
    "artist_table_df.printSchema()\n",
    "artist_table_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Load Log Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# https://thispointer.com/python-how-to-unzip-a-file-extract-single-multiple-or-all-files-from-a-zip-archive/\n",
    "# Unzip the log_data file into new folder\n",
    "with ZipFile('data/log-data.zip', 'r') as zipObj:\n",
    "   # Extract all the contents of zip file in current directory\n",
    "   zipObj.extractall('data/log-data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "log_data_df = spark.read.json(\"data/log-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "log_data_df.printSchema()\n",
    "log_data_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "log_data_filtered_df = log_data_df.filter(log_data_df.page == 'NextSong')\n",
    "\n",
    "log_data_filtered_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "log_data_filtered_df.createOrReplaceTempView(\"user_table_DF\")\n",
    "\n",
    "user_table = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT int(userId) as user_id,\n",
    "                    firstName as first_name,\n",
    "                    lastName as last_name,\n",
    "                    gender,\n",
    "                    level\n",
    "    FROM user_table_DF\n",
    "    ORDER BY user_id\n",
    "                    \"\"\")\n",
    "user_table.printSchema()\n",
    "user_table.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "user_table.write.mode(\"overwrite\").partitionBy(\"user_id\").parquet(\"/data/output_data/user_table.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "user_table = spark.read.parquet('/data/output_data/user_table.parquet')\n",
    "\n",
    "user_table.printSchema()\n",
    "user_table.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/51983037/convert-from-timestamp-to-specific-date-in-pyspark\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from datetime import datetime\n",
    "from pyspark.sql import types as t\n",
    "# Create a function that returns the desired string from a timestamp\n",
    "\n",
    "def format_timestamp(ts):\n",
    "    return datetime.fromtimestamp(ts / 1000.0)\n",
    "\n",
    "# Create the UDF\n",
    "format_timestamp_udf = udf(lambda x: format_timestamp(x), t.TimestampType())\n",
    "\n",
    "# Finally, apply the function to each element of the 'timestamp' column\n",
    "log_data_filtered_df = log_data_filtered_df.withColumn('timestamp', format_timestamp_udf(log_data_filtered_df['ts']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "log_data_filtered_df.printSchema()\n",
    "log_data_filtered_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/51983037/convert-from-timestamp-to-specific-date-in-pyspark\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from datetime import datetime\n",
    "from pyspark.sql import types as t\n",
    "# Create a function that returns the desired string from a timestamp\n",
    "\n",
    "def format_datetime(ts):\n",
    "    return datetime.fromtimestamp(ts / 1000.0).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Create the UDF\n",
    "format_datetime_udf = udf(lambda x: format_datetime(x), t.StringType())\n",
    "\n",
    "# Finally, apply the function to each element of the 'timestamp' column\n",
    "log_data_filtered_df = log_data_filtered_df.withColumn('datetime', format_datetime_udf(log_data_filtered_df['ts']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "log_data_filtered_df.printSchema()\n",
    "log_data_filtered_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "log_data_filtered_df.createOrReplaceTempView(\"time_table_DF\")\n",
    "\n",
    "time_table = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT datetime as start_time,\n",
    "            hour(timestamp) as hour,\n",
    "            day(timestamp) as day,\n",
    "            weekofyear(timestamp) as week,\n",
    "            month(timestamp) as month,\n",
    "            year(timestamp) as year,\n",
    "            dayofweek(timestamp) as weekday\n",
    "    FROM time_table_DF\n",
    "    ORDER BY start_time \"\"\")\n",
    "\n",
    "time_table.printSchema()\n",
    "time_table.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "time_table.write.mode(\"overwrite\").partitionBy(\"year\", \"month\").parquet(\"/data/output_data/time_table.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "time_table = spark.read.parquet(\"/data/output_data/time_table.parquet\")\n",
    "\n",
    "time_table.printSchema()\n",
    "time_table.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# https://sparkbyexamples.com/pyspark/pyspark-join-explained-with-examples/\n",
    "\n",
    "song_log_join_df = song_data_df.join(log_data_filtered_df, (log_data_filtered_df.artist == song_data_df.artist_name) & (log_data_filtered_df.song == song_data_df.title))\n",
    "\n",
    "song_log_join_df.printSchema()\n",
    "song_log_join_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/46213986/how-could-i-add-a-column-to-a-dataframe-in-pyspark-with-incremental-values\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "song_log_join_df = song_log_join_df.withColumn(\"songplay_id\", monotonically_increasing_id())\n",
    "\n",
    "song_log_join_df.createOrReplaceTempView('songplay_table_df')\n",
    "\n",
    "songplay_table = spark.sql(\"\"\"\n",
    "    SELECT  songplay_id,\n",
    "            datetime as start_time,\n",
    "            userId as user_id,\n",
    "            level,\n",
    "            song_id,\n",
    "            artist_id,\n",
    "            sessionId as session_id,\n",
    "            location,\n",
    "            userAgent as user_agent\n",
    "    FROM songplay_table_df\n",
    "    ORDER BY songplay_id\n",
    "            \"\"\")\n",
    "\n",
    "songplay_table.printSchema()\n",
    "songplay_table.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
